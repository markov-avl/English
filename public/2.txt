THE MEANING OF PARALLEL PROGRAMMING

Figure 1 is, in fact, rather pointless because there is no need to draw a flow diagram in such detail in these simple cases. The branching and re-combining of the flow paths is done automatically in a way which need not worry the programmer. Also one of the branches is very special and simple, requiring only a rudimentary control unit to carry it out.

It may be asked whether a situation ever arises in which both of two simultaneous branches of a program are of a sufficient complexity to justify the adoption of programming by means of a series of instructions rather than a single initiating instruction. The answer to this question is undoubtedly yes. Since in such a situation there are two activities proceeding in parallel, each defined by a program of instructions, the type of programming involved will be referred to as Parallel Programming.

Parallel Programming will be used in both the Sperry Rand LARC (Eckert, 1956) and the IBM Stretch (Dunwell, 1956). Both of these machines will contain special computers for handling the input and output of information, which will be programmed by means of series of instructions. Furthermore, LARC has provision for two main computers operating together.

It must be noted that this situation is quite different from having two entirely separate computers standing side by side. The computers within LARC or Stretch both have access to the same store and can therefore participate in the same calculation. Consequently there is a need for control links to prevent one computer anticipating another. All this will raise some fascinating problems in programming. These problems may be divided into three types:
(1) problems of flow charting;
(2) programming the actual links which keep the control units from anticipating one another;
(3) allocating storage space to the various parts of the program and batches of data.

As yet very little work has been done on these problems. The purpose of this paper is merely to define the area where investigation is needed, and to point out some of the possible consequences of adopting the technique of parallel programming.

TIME-SHARING

So far the subject of parallel programming has been introduced only in situations where there are two or more separate parts of a machine, each dealing with its own branch of the program. However, very similar logical problems of programming can occur in a situation which does not involve more than one control unit. This comes about by allowing a single control unit to divide its time between two different activities. Strictly speaking the control unit does not obey the two different parts of the program simultaneously, but if it switches frequently from one part to another the overall effect is very similar to having the two parts executed simultaneously. Figure 2 indicates in a crude way how control might proceed.

At first sight there is no point in introducing the of time-sharing, since if the two parts of the program were executed one after another at full speed this could not take any longer than obeying them in any other sequence. In practice, however, there may be a very real advantage in time-sharing. This is because one of the branches may occasionally be held up while it is waiting for some piece of peripheral equipment to become ready to operate. In normal programming the computer must then remain idle for a period, but if time-sharing is being employed it can be arranged that the control unit proceeds to another branch of the program which is not held up in the same way.

Time-sharing is not an entirely new idea. Many people must have written programs using conventional programming methods, in which the time which the computer might have spent idly waiting for a peripheral device to operate would have been serious, and in which this time was therefore occupied with snatches of some other part of the program. However, writing such programs is not easy. It is necessary to estimate approximately how much time needs to be occupied on each occasion, and somehow to arrange the program so that approximately the right amount of work is fitted into these periods. The timing of loops, etc, in the calculation may make such programming exceedingly difficult.

Time-sharing can be useful, for example, in controlling the positioning of magnetic tape. It is often assumed, in this country at least, that a requirement for a magnetic tape unit is that it shall be able to position the tape at any block whose address is given at random. In general this positioning process may take some seconds, and it is therefore desirable for a normal computation to proceed at the same time if possible. However, the positioning process is somewhat elaborate in that it involves determining the appropriate direction of motion, computing the number of blocks to be passed, counting these blocks as they pass, and probably checking block addresses on the way. Hence any special purpose control unit that is built to do this job must be rather more than rudimentary. However, positioning a magnetic tape is not a full-time occupation for a computing device. The necessary arithmetic can, in fact, be done electronically in only a small fraction of the time actually taken to do the positioning. Hence if the computer itself can be made to divide its time between attending to the position of the tape and executing a calculation, the latter calculation can proceed at almost its normal speed.

This idea was actually used with EDSAC I at the Cambridge University Mathematical Laboratory (Wilkes and Willis, 1956). Special routines were written to count the passage of blocks on the tape and to control its movement. One of these routines was included in every program which made use of the magnetic tape equipment. As every block passed the reading head, a special block marker caused a flip-flop inside the machine to be set. Each time this happened, control was transferred to the positioning routine. After this routine had taken account of the block, and performed any other necessary action, it returned control to the main program.

In this case it was not necessary for the programmer to design his program so that it fitted into the gaps between the block counting operations. Instead, the machine proceeded at full speed, only switching to the positioning routine when necessary; in fact, it was not possible for the programmer to predict in advance the exact points at which his program would be interrupted by the positioning routine. The means adopted to bring this about were quite conventional. A conditional jump instruction was provided whose action depended on the setting of the fip-fop which recorded the passage of block markers. This instruction was used in such a way that the first time it was encountered after a block had passed, it caused control to be transferred to the positioning routine. The programmer was required to scatter copies of this instruction throughout his program in such a way that the interval of time between any two consecutive encounters with them would certainly be less than the time taken for one block to pass through the tape mechanism. This was, needless to say, a tiresome requirement. It did, however, enable fairly efficient use to be made of the computer while the tape was being positioned.

AUTOMATIC INTERRUPTION

There is no fundamental difficulty involved in designing a machine so that the execution of instructions can be interrupted automatically as a result of some external signal. When this is done, efficient time-sharing can be achieved without clumsy programming tricks. EDSAC 2 will in fact interrupt itself in order to count the passage of magnetic tape blocks. However, in the future one computer in particular will stand out as a classic example of the wholesale application of the time-sharing technique. This is the TX-2 computer (Papian, 1957) now nearing completion at the Lincoln Laboratory of the Massachusetts Institute of Technology.

TX-2 will have special facilities for time-sharing between up to 33 branches of a program, and it is expected that quite a large number of these will in fact be used. In general each branch will deal with one piece of peripheral equipment. The time-sharing will be done on a priority basis, the branches dealing with the fastest devices (for example, magnetic tape) getting top priority, and the slower devices (for example, the paper tape punch) getting lower priority. The lowest priority of all goes to the main calculation itself. This contains no reference to any peripheral equipment and can therefore be fitted in at any convenient time. Elaborate built-in circuits will continually look for the highest priority branch which is requiring attention at any moment, and will take care of the switching of control from one branch to another.

For example, suppose that the calculation arrives at a point at which some results are ready to be recorded on magnetic tape. Then a branch of the program dealing with magnetic tape output is started up, and since this has high priority it immediately assumes control of the machine. It prepares the first piece of information and sends it to the magnetic tape, prepares the second, and then can do no more because the magnetic tape equipment is still occupied with the first piece of information. Control therefore lapses to the main program, which now embarks on another part of the calculation. As soon as the magnetic tape is ready to receive the second piece of information, the main program is interrupted and control is again transferred to the magnetic tape branch. Thus, so long as the magnetic tape is in operation, control of the machine will oscillate between the two branches of the program.

In addition, while all this is going on, the machine may also be punching some information on paper tape. There will then be three branches of the program all requiring attention from time to time, and control may jump from one to another in a way indicated in Figure 3(a).


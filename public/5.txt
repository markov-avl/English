Web Semantics in the Clouds

In the last two years, the amount of structured data made available on the Web in semantic formats has grown by several orders of magnitude. On one side, the Linked Data effort has made available online hundreds of millions of entity descriptions based on the Resource Description Framework (RDF) in data sets such as DBPedia, Uniprot, and Geonames. On the other hand, the Web 2-0 community has increasingly embraced the idea of data portability, and the first efforts have already produced ­billions of RDF-equivalent triples either embedded inside HTML pages using microformats or exposed directly using eRDF (embedded RDF) and RDFa (RDF attributes).

Incentives for exposing such data are also finally becoming clearer. Yahoo’s SearchMonkey, for example, makes Web sites containing structured data stand out from others by providing the most appropriate visualization for the end user in the search result page. It will not be long, we envision, before search engines will also directly use this information for ranking and relevance purposes—returning, for example, qualitatively better results for queries that involve everyday entities such as events, locations, and people.

Even though we’re still at the beginning of the data Web era, the amount of information already available is clearly much larger than what could be contained, for example, in any current-generation triple store (a database for storing and retrieving RDF metadata) typically running on single servers.

Although many applications will need to work with large amounts of metadata, one particular application would certainly not exist without the capability of accessing and processing arbitrary amounts of metadata: search engines that locate the data and services that other applications need. For this reason, Semantic Web search engines and large-scale services are now the first in harnessing grid computing’s power when it comes to scaling far beyond the current generation of triple stores.

Cloud computing for the web of data

Not all computationally intense problems require similarly structured hardware configurations. Classic super-computers, typically characterized by superior arithmetic performance per CPU and high-end CPU interconnection technologies, are proven tools that have scored great successes in physics, astronomy, chemistry, biology, and many other fields. In general, researchers find it hard to surpass such high-end machines when they're facing problems that tend to be hard to parallelize or when they need intense interprocess communication.

When this isn't the case, however, cluster-computing approaches typically exhibit far greater flexibility in resource utilization and much lower overall costs. In an extreme case, clouds can extend to the entire Internet: computations involving relatively small data blocks, with no coordination needs and no significant constraints on execution times, have been performed across the Internet - for example, using free desktop cycles such as in the SETI or Folding@ projects.

The computations needed in Web data processing lie somewhere between these extremes. On the one hand, Web data is interlinked, and the analysis of its mesh has proven to be fundamental in gaining insights about its implicit nature. At the same time, however, the data is by nature distributed and can be said to be consistent with itself, if at all, only within the boundaries of a single Web site. Furthermore, a prominent characteristic is definitely the sheer amount of such data, with a petabyte being a common order of magnitude.

To process this kind of data, leading Internet search providers have been pioneering ways to perform largescale Web data computations on clusters of commodity machines interconnected with mainstream networking technology. Google's publications about its MapReduce framework and more recently the Yahoo-initiated, open source implementation Hadoop are attracting increasing attention from developers and users alike.

The MapReduce style of computation works well with the possibilities offered by the emerging cloud-computing paradigm. This computation style provides generally useful abstractions so that developers can focus on the task at hand (see the sidebar "More about These Technologies"). Executing computations "in the clouds" refers to the model in which an application requests computational resources from a service provider without needing to bother with the computational offer's details. An example that's recently enjoying popularity is the Amazon Elastic Computing service offer, which allows computing capabilities to be allocated within minutes and increased dynamically as required-for example, to quickly cope with an unexpected peak of visitors. Because MapReduce is agnostic with regard to the actual size of the cluster it runs on, executing Hadoop on such cloud-like infrastructures is an appealing strategy for performing data-intensive computations while minimizing, or optimizing, the upfront infrastructure investment. The combined paradigm is generally referred to as data-intensive scalable (or super) computing (DISC).

With respect to the complex tasks involved in processing the Web of data, a DISC approach is for many reasons a natural choice. On the one hand, many tasks, such as crawling, can be performed in a fashion similar to the processing of regular Web content (HTML pages). Semantic data crawling often requires special treatment and dedicated intelligence but doesn't generally differ very much from crawling the HTML Web. Similarly, ranking a Semantic Web source such as sites or data sets on the basis of algorithms similar to PageRank, and thus efficiently computed over MapReduce, has been proposed. On the other hand, many other data-intensive, batch-processing tasks are needed for specifically addressing the challenges of the Web of data. Examples include large-scale data analysis, cleaning, reasoning, entity recognition and consolidation, and ontology mapping.

Grid computing is certainly useful in some of these tasks. We first show how Yahoo is building on grid computing using Hadoop to enable the analysis, transformation, and querying of large amounts of RDF data in a batch-processing mode using clusters of hundreds of machines without apparent bottlenecks in scalability. Next, we show how the Semantic Web search engine Sindice is exploiting Hadoop and related technologies to scale semantic indexing beyond the limits of dedicated cluster environments while reducing cost and complexity.

Batch-processing RDF using Yahoo Pig

Yahoo is building on grid computing using Hadoop to enable the analysis, transformation, and querying of large amounts of RDF data in a batch-processing mode using clusters of hundreds of machines without apparent bottlenecks in scalability. The Yahoo crawler affectionately named Slurp began indexing microformat content in the spring of this year, and the company recently added eRDF and RDFa to its supported formats. Yahoo has also innovated in the Semantic Web area by allowing site owners to expose metadata using the DataRSS format, an Atom-based format for delivering RDF data. The Yahoo SearchMonkey application platform will likely produce a further explosion in the amount of data handled. SearchMonkey developers can create so-called custom data services to extract metadata from existing Web sites or turn APIs into sources of metadata. All the metadata collected by Yahoo is stored in an RDF-compatible format, so processing it requires the ability to query and transform large amounts of RDF data.

Yahoo aims to develop data solutions that address wide classes of data management problems and that can easily be adapted to new problems, such as the one posed by SearchMonkey. One of these tools, Yahoo Pig, simplifies the processing of large data sets on computer clusters by applying concepts from parallel databases. It was originally developed inside Yahoo Research but has been recently made available as open source under the Apache 2-0 license. Pig natively provides support for data transformations such as projections, grouping, sorting, joining, and compositions. The expressivity of Pig's transformation language is roughly equivalent to standard relational algebra (which also forms the basis of SOL), with the added benefit of extensibility through custom functions written in Java. Pig programmers develop custom code for loading and saving data in other formats into Pig's data model, which again builds on the relational model (bags of tuples) with additional features such as maps and nested bags of tuples. Scripts written in PigLatin, Pig's native language, are executed on the cluster using the Hadoop framework or Galago, a tuple-processing engine. In contrast to HBase, Pig can't actually be called a database: processing takes place by iterating through the whole data set (the data isn't indexed and can't be updated), and the results of computation are saved. HBase, however, doesn't offer a query language, only the retrieval of tuples by their index.

We observed that Pig's data model and transformation language are similar to the relational representations of RDF and the Sparql query language, respectively, so we recently extended Pig to perform RDF querying and transformations. As part of this work, we implemented load and save functions to convert RDF to Pig's data model, created a mapping between Sparql and PigLatin, and proved that the mapping is complete.
